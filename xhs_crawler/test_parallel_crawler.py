#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œçˆ¬è™«æ¨¡å—æµ‹è¯•è„šæœ¬
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯•å¹¶è¡Œçˆ¬è™«æ¨¡å—å¯¼å…¥")
    print("=" * 60)
    
    try:
        from xhs_crawler.crawlers.parallel_keyword_crawler import (
            ParallelKeywordCrawler,
            run_parallel_crawler,
            CrawlResult
        )
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        print(f"  - ParallelKeywordCrawler: {ParallelKeywordCrawler.__name__}")
        print(f"  - run_parallel_crawler: {run_parallel_crawler.__name__}")
        print(f"  - CrawlResult: {CrawlResult.__name__}")
        return True
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_crawler_instantiation():
    """æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ–")
    print("=" * 60)
    
    try:
        from xhs_crawler.crawlers.parallel_keyword_crawler import ParallelKeywordCrawler
        
        crawler = ParallelKeywordCrawler(
            max_workers=3,
            detail_concurrency=5,
            timeout_per_keyword=120.0,
            timeout_per_detail=30.0
        )
        print("âœ… ParallelKeywordCrawler å®ä¾‹åŒ–æˆåŠŸ")
        print(f"  - max_workers: {crawler.max_workers}")
        print(f"  - detail_concurrency: {crawler.detail_concurrency}")
        print(f"  - timeout_per_keyword: {crawler.timeout_per_keyword}")
        print(f"  - timeout_per_detail: {crawler.timeout_per_detail}")
        return True
    except Exception as e:
        print(f"âŒ å®ä¾‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_crawl_result_dataclass():
    """æµ‹è¯•CrawlResultæ•°æ®ç±»"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•CrawlResultæ•°æ®ç±»")
    print("=" * 60)
    
    try:
        from xhs_crawler.crawlers.parallel_keyword_crawler import CrawlResult
        
        result = CrawlResult(
            keyword="æµ‹è¯•å…³é”®è¯",
            success=True,
            notes=[{"note_id": "123", "title": "æµ‹è¯•ç¬”è®°"}],
            duration=10.5,
            pages_crawled=3
        )
        print("âœ… CrawlResult æ•°æ®ç±»æµ‹è¯•æˆåŠŸ")
        print(f"  - keyword: {result.keyword}")
        print(f"  - success: {result.success}")
        print(f"  - notes count: {len(result.notes)}")
        print(f"  - duration: {result.duration}")
        print(f"  - pages_crawled: {result.pages_crawled}")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®ç±»æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    results = []
    
    results.append(("æ¨¡å—å¯¼å…¥æµ‹è¯•", test_imports()))
    
    if results[-1][1]:
        results.append(("çˆ¬è™«å®ä¾‹åŒ–æµ‹è¯•", test_crawler_instantiation()))
    
    if results[-1][1]:
        results.append(("æ•°æ®ç±»æµ‹è¯•", test_crawl_result_dataclass()))
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)
