#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¢é‡çˆ¬å–æ•°æ®åº“é›†æˆåŠŸèƒ½
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from xhs_crawler.core.incremental_crawler import (
    get_incremental_crawler,
    ContentFingerprint,
    filter_duplicate_posts
)


def test_database_integration():
    """æµ‹è¯•æ•°æ®åº“é›†æˆåŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•å¢é‡çˆ¬å–æ•°æ®åº“é›†æˆåŠŸèƒ½")
    print("=" * 60)
    
    crawler = get_incremental_crawler()
    
    print("\n1. æµ‹è¯•æ¸…ç©ºç°æœ‰æŒ‡çº¹...")
    crawler.clear_fingerprints()
    
    print("\n2. æµ‹è¯•ä¿å­˜å†…å®¹æŒ‡çº¹åˆ°æ•°æ®åº“...")
    test_fingerprints = [
        ContentFingerprint(
            note_id="test_note_001",
            content_hash="abc123hash",
            title_hash="def456hash",
            combined_hash="combined789hash",
            title="Pythonç®—æ³•æ•™ç¨‹",
            content_preview="æœ¬æ–‡ä»‹ç»Pythonä¸­çš„æ’åºç®—æ³•..."
        ),
        ContentFingerprint(
            note_id="test_note_002",
            content_hash="xyz789hash",
            title_hash="uvw012hash",
            combined_hash="combined345hash",
            title="æœºå™¨å­¦ä¹ å…¥é—¨",
            content_preview="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯..."
        )
    ]
    
    saved_count = crawler.save_fingerprints_batch(test_fingerprints, source="test")
    print(f"   ä¿å­˜æŒ‡çº¹æ•°é‡: {saved_count}")
    
    print("\n3. æµ‹è¯•ä»æ•°æ®åº“åŠ è½½ç°æœ‰æŒ‡çº¹...")
    loaded_count = crawler.load_existing_fingerprints(source="test")
    print(f"   åŠ è½½æŒ‡çº¹æ•°é‡: {loaded_count}")
    
    print("\n4. æµ‹è¯•é‡å¤æ£€æµ‹...")
    result1 = crawler.check_duplicate(
        note_id="test_note_003",
        title="Pythonç®—æ³•æ•™ç¨‹",
        content="æœ¬æ–‡ä»‹ç»Pythonä¸­çš„æ’åºç®—æ³•..."
    )
    print(f"   ç›¸ä¼¼å†…å®¹æ£€æµ‹ç»“æœ: duplicate_type={result1.duplicate_type}, is_duplicate={result1.is_duplicate}")
    
    result2 = crawler.check_duplicate(
        note_id="test_note_004",
        title="å…¨æ–°çš„ç®—æ³•æ–‡ç« ",
        content="è¿™æ˜¯ä¸€ç¯‡å…³äºæ•°æ®ç»“æ„çš„æ–‡ç« ..."
    )
    print(f"   æ–°å†…å®¹æ£€æµ‹ç»“æœ: duplicate_type={result2.duplicate_type}, is_duplicate={result2.is_duplicate}")
    
    print("\n5. æµ‹è¯•è·å–é‡å¤ç»Ÿè®¡ä¿¡æ¯...")
    stats = crawler.get_duplicate_stats(source="test")
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    print("\n6. æµ‹è¯•æœ¬åœ°å†…å­˜ç»Ÿè®¡...")
    local_stats = crawler.get_statistics()
    print(f"   æœ¬åœ°ç»Ÿè®¡: {local_stats}")
    
    print("\n7. æµ‹è¯•æ‰¹é‡ä¿å­˜æ–°æ£€æµ‹çš„å†…å®¹...")
    new_fingerprints = []
    if result2.fingerprint:
        new_fingerprints.append(result2.fingerprint)
    
    if new_fingerprints:
        saved_count = crawler.save_fingerprints_batch(new_fingerprints, source="test")
        print(f"   ä¿å­˜æ–°æŒ‡çº¹æ•°é‡: {saved_count}")
    
    print("\n8. æµ‹è¯•æ¸…ç†è¿‡æœŸæŒ‡çº¹...")
    deleted_count = crawler.cleanup_old_fingerprints(days=1)
    print(f"   æ¸…ç†è¿‡æœŸæŒ‡çº¹æ•°é‡: {deleted_count}")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®åº“é›†æˆæµ‹è¯•å®Œæˆ")
    print("=" * 60)


def test_content_fingerprint_creation():
    """æµ‹è¯•å†…å®¹æŒ‡çº¹åˆ›å»º"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å†…å®¹æŒ‡çº¹åˆ›å»ºåŠŸèƒ½")
    print("=" * 60)
    
    crawler = get_incremental_crawler()
    crawler.clear_fingerprints()
    
    test_cases = [
        {
            "note_id": "note_001",
            "title": "  Python  é«˜çº§  ç‰¹æ€§  ",
            "content": "  è¿™æ˜¯ä¸€ç¯‡å…³äºPythonçš„æ–‡ç« ï¼ŒåŒ…å«å¾ˆå¤šç©ºæ ¼  "
        },
        {
            "note_id": "note_002",
            "title": "Pythoné«˜çº§ç‰¹æ€§",
            "content": "è¿™æ˜¯ä¸€ç¯‡å…³äºPythonçš„æ–‡ç« ï¼ŒåŒ…å«å¾ˆå¤šç©ºæ ¼"
        }
    ]
    
    for case in test_cases:
        fingerprint = crawler._create_fingerprint(
            case["note_id"],
            case["title"],
            case["content"]
        )
        print(f"\n  ç¬”è®°ID: {case['note_id']}")
        print(f"  æ ‡é¢˜å“ˆå¸Œ: {fingerprint.title_hash[:16]}...")
        print(f"  å†…å®¹å“ˆå¸Œ: {fingerprint.content_hash[:16]}...")
        print(f"  ç»„åˆå“ˆå¸Œ: {fingerprint.combined_hash[:16]}...")
    
    print("\n  æµ‹è¯•ä¸¤ä¸ªç›¸ä¼¼å†…å®¹çš„å“ˆå¸Œæ˜¯å¦ä¸åŒï¼ˆå¸¦ç©ºæ ¼å½’ä¸€åŒ–ï¼‰:")
    print(f"    æ¡ˆä¾‹1æ ‡é¢˜å“ˆå¸Œ: {crawler.content_fingerprints['note_001'].title_hash}")
    print(f"    æ¡ˆä¾‹2æ ‡é¢˜å“ˆå¸Œ: {crawler.content_fingerprints['note_002'].title_hash}")
    print(f"    å“ˆå¸Œæ˜¯å¦ç›¸åŒ: {crawler.content_fingerprints['note_001'].title_hash == crawler.content_fingerprints['note_002'].title_hash}")
    
    print("\n" + "=" * 60)
    print("âœ… å†…å®¹æŒ‡çº¹åˆ›å»ºæµ‹è¯•å®Œæˆ")
    print("=" * 60)


def test_similarity_detection():
    """æµ‹è¯•ç›¸ä¼¼åº¦æ£€æµ‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç›¸ä¼¼åº¦æ£€æµ‹åŠŸèƒ½")
    print("=" * 60)
    
    crawler = get_incremental_crawler()
    crawler.clear_fingerprints()
    crawler.set_similarity_threshold(0.7)
    
    base_content = "Pythonæ˜¯ä¸€é—¨æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸã€‚"
    
    similar_cases = [
        {"note_id": "base_001", "title": "Pythonç¼–ç¨‹è¯­è¨€ä»‹ç»", "content": base_content},
        {"note_id": "similar_001", "title": "Pythonç¼–ç¨‹è¯­è¨€å…¥é—¨", "content": "Pythonæ˜¯ä¸€é—¨æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸã€‚"},
        {"note_id": "different_001", "title": "JavaScriptæ•™ç¨‹", "content": "JavaScriptæ˜¯ä¸€é—¨ç”¨äºWebå‰ç«¯å¼€å‘çš„è„šæœ¬è¯­è¨€ã€‚"},
    ]
    
    for case in similar_cases:
        result = crawler.check_duplicate(
            case["note_id"],
            case["title"],
            case["content"],
            check_exact=True,
            check_similar=True
        )
        print(f"\n  ç¬”è®°ID: {case['note_id']}")
        print(f"  é‡å¤ç±»å‹: {result.duplicate_type}")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {result.similarity_score}")
        print(f"  æ˜¯å¦é‡å¤: {result.is_duplicate}")
        print(f"  é‡å¤IDåˆ—è¡¨: {result.duplicate_note_ids}")
    
    print("\n" + "=" * 60)
    print("âœ… ç›¸ä¼¼åº¦æ£€æµ‹æµ‹è¯•å®Œæˆ")
    print("=" * 60)


def test_batch_filter():
    """æµ‹è¯•æ‰¹é‡è¿‡æ»¤åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ‰¹é‡è¿‡æ»¤åŠŸèƒ½")
    print("=" * 60)
    
    crawler = get_incremental_crawler()
    crawler.clear_fingerprints()
    
    posts = [
        {"note_id": "post_001", "title": "ç®—æ³•ä¹‹ç¾", "content": "æœ¬æ–‡æ¢è®¨ç®—æ³•çš„ä¼˜é›…ä¸æ•ˆç‡ã€‚"},
        {"note_id": "post_002", "title": "æ•°æ®ç»“æ„çš„é­…åŠ›", "content": "æ•°æ®ç»“æ„æ˜¯ç¨‹åºè®¾è®¡çš„åŸºçŸ³ã€‚"},
        {"note_id": "post_003", "title": "ç®—æ³•ä¹‹ç¾", "content": "æœ¬æ–‡æ¢è®¨ç®—æ³•çš„ä¼˜é›…ä¸æ•ˆç‡ã€‚"},  # é‡å¤
        {"note_id": "post_004", "title": "æ’åºç®—æ³•è¯¦è§£", "content": "ä»‹ç»å¸¸è§çš„æ’åºç®—æ³•åŠå…¶å®ç°ã€‚"},
        {"note_id": "post_005", "title": "ç®—æ³•ä¹‹ç¾", "content": "æœ¬æ–‡æ¢è®¨ç®—æ³•çš„ä¼˜é›…ä¸æ•ˆç‡ã€‚"},  # é‡å¤
    ]
    
    new_posts, duplicate_posts = filter_duplicate_posts(posts, threshold=0.85)
    
    print(f"\n  åŸå§‹å¸–å­æ•°é‡: {len(posts)}")
    print(f"  æ–°å¸–å­æ•°é‡: {len(new_posts)}")
    print(f"  é‡å¤å¸–å­æ•°é‡: {len(duplicate_posts)}")
    
    print("\n  æ–°å¸–å­åˆ—è¡¨:")
    for post in new_posts:
        print(f"    - {post['note_id']}: {post['title']}")
    
    print("\n  é‡å¤å¸–å­åˆ—è¡¨:")
    for post in duplicate_posts:
        print(f"    - {post['note_id']}: {post['title']} (ç±»å‹: {post['duplicate_type']})")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰¹é‡è¿‡æ»¤æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    try:
        test_database_integration()
        test_content_fingerprint_creation()
        test_similarity_detection()
        test_batch_filter()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
