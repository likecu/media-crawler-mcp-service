#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI/MLå·¥å…·æ¨¡å—ï¼Œæä¾›å¢å¼ºçš„å†…å®¹åˆ†æå’Œæ¨èåŠŸèƒ½
ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒå¼‚æ­¥æ¨ç†ã€æ‰¹é‡å¤„ç†ã€ç»“æœç¼“å­˜
"""

import os
import sys
import json
import re
import time
import hashlib
import asyncio
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
import threading
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from xhs_crawler.core.mcp_utils import MCPUtils
from xhs_crawler.core.database import get_neon_database

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from xhs_crawler.core.config import get_output_dir, get_detail_dir, OCR_CONFIG

OCR_TOOL = OCR_CONFIG["tool_path"]


@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœæ•°æ®ç±»"""
    content: str
    summary: str = ""
    sentiment: str = "ä¸­æ€§"
    key_points: List[str] = field(default_factory=list)
    category: str = "æœªåˆ†ç±»"
    difficulty: str = "ä¸­çº§"
    confidence: float = 0.0
    tokens_used: int = 0
    inference_time: float = 0.0
    cached: bool = False


class LRUCache:
    """LRUç¼“å­˜å®ç°"""
    
    def __init__(self, capacity: int = 1000, ttl: int = 3600):
        """
        åˆå§‹åŒ–LRUç¼“å­˜
        
        Args:
            capacity: æœ€å¤§ç¼“å­˜æ•°é‡
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.capacity = capacity
        self.ttl = ttl
        self.cache = {}
        self.access_order = {}
        self.lock = threading.Lock()
        self._counter = 0
    
    def _generate_key(self, content: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get(self, content: str) -> Optional[InferenceResult]:
        """
        è·å–ç¼“å­˜ç»“æœ
        
        Args:
            content: è¾“å…¥å†…å®¹
            
        Returns:
            ç¼“å­˜çš„æ¨ç†ç»“æœï¼Œå¦‚æœæœªå‘½ä¸­è¿”å›None
        """
        with self.lock:
            key = self._generate_key(content)
            
            if key not in self.cache:
                return None
            
            result, timestamp = self.cache[key]
            if datetime.now() - timestamp > timedelta(seconds=self.ttl):
                del self.cache[key]
                del self.access_order[key]
                return None
            
            self._counter += 1
            self.access_order[key] = self._counter
            return result
    
    def set(self, content: str, result: InferenceResult):
        """
        è®¾ç½®ç¼“å­˜ç»“æœ
        
        Args:
            content: è¾“å…¥å†…å®¹
            result: æ¨ç†ç»“æœ
        """
        with self.lock:
            key = self._generate_key(content)
            self._counter += 1
            
            if key in self.cache:
                del self.cache[key]
                del self.access_order[key]
            
            if len(self.cache) >= self.capacity:
                oldest_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])
                del self.cache[oldest_key]
                del self.access_order[oldest_key]
            
            self.cache[key] = (result, datetime.now())
            self.access_order[key] = self._counter
    
    def clear_expired(self):
        """æ¸…é™¤è¿‡æœŸç¼“å­˜"""
        with self.lock:
            now = datetime.now()
            expired_keys = [
                k for k, (v, t) in self.cache.items()
                if now - t > timedelta(seconds=self.ttl)
            ]
            for k in expired_keys:
                del self.cache[k]
                del self.access_order[k]


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ï¼Œæ”¯æŒå¹¶å‘æ‰¹é‡æ¨ç†"""
    
    def __init__(self, max_concurrent: int = 5, batch_size: int = 10):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent)
    
    async def process_batch(
        self,
        items: List[Dict[str, Any]],
        process_func: callable,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†é¡¹ç›®
        
        Args:
            items: é¡¹ç›®åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°
            kwargs: å¤„ç†å‡½æ•°å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        batches = [
            items[i:i + self.batch_size]
            for i in range(0, len(items), self.batch_size)
        ]
        
        for batch in batches:
            tasks = [
                self._process_with_semaphore(process_func, item, **kwargs)
                for item in batch
            ]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(batch_results)
        
        return results
    
    async def _process_with_semaphore(
        self,
        process_func: callable,
        item: Dict[str, Any],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘å¤„ç†å•ä¸ªé¡¹ç›®
        
        Args:
            process_func: å¤„ç†å‡½æ•°
            item: é¡¹ç›®
            kwargs: å¤„ç†å‡½æ•°å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        async with self.semaphore:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self.executor,
                lambda: process_func(item, **kwargs)
            )


class AIUtils:
    """
    AI/MLå·¥å…·ç±»ï¼Œæä¾›å¢å¼ºçš„å†…å®¹åˆ†æå’Œæ¨èåŠŸèƒ½
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒå¼‚æ­¥æ¨ç†ã€æ‰¹é‡å¤„ç†ã€ç»“æœç¼“å­˜
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–AIå·¥å…·"""
        if self._initialized:
            return
        
        self.mcp_utils = MCPUtils()
        self.tfidf_vectorizer = TfidfVectorizer(stop_words=None)
        self.post_vectors = None
        self.posts = None
        self.cache = LRUCache(capacity=500, ttl=1800)
        self.batch_processor = BatchProcessor(max_concurrent=3, batch_size=5)
        self._initialized = True
    
    def summarize_content_enhanced(
        self,
        content: str,
        title: str,
        images: List[Dict[str, Any]] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        å¢å¼ºçš„å†…å®¹æ€»ç»“åŠŸèƒ½ï¼ŒåŒ…æ‹¬å†…å®¹æ€»ç»“ã€æƒ…æ„Ÿåˆ†æå’Œå…³é”®ä¿¡æ¯æå–
        
        Args:
            content: å¸–å­å†…å®¹
            title: å¸–å­æ ‡é¢˜
            images: å›¾ç‰‡åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åŒ…å«æ€»ç»“ã€æƒ…æ„Ÿåˆ†æå’Œå…³é”®ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹å¢å¼ºå†…å®¹æ€»ç»“: '{title[:30]}...'")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        try:
            cache_key = f"{title[:50]}|{content[:100]}"
            
            if use_cache:
                cached_result = self.cache.get(cache_key)
                if cached_result:
                    print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ")
                    return {
                        "summary": cached_result.summary,
                        "sentiment": cached_result.sentiment,
                        "key_points": cached_result.key_points,
                        "category": cached_result.category,
                        "difficulty": cached_result.difficulty,
                        "confidence": cached_result.confidence,
                        "cached": True
                    }
            
            full_content = content
            if images and len(images) > 0:
                print(f"ğŸ“¸ åŒ…å« {len(images)} å¼ å›¾ç‰‡")
            
            start_time = time.time()
            
            question = f'''è¯·å¯¹è¿™ç¯‡å†…å®¹è¿›è¡Œå¢å¼ºæ€»ç»“ï¼Œè¾“å‡ºæ ¼å¼ä¸ºJSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - summary: ä¸»è¦å†…å®¹æ€»ç»“ï¼ˆ200å­—ä»¥å†…ï¼‰
            - sentiment: æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/ä¸­æ€§/æ¶ˆæï¼‰
            - key_points: å…³é”®ä¿¡æ¯åˆ—è¡¨ï¼ˆ5-10ä¸ªè¦ç‚¹ï¼‰
            - category: å†…å®¹ç±»åˆ«
            - difficulty: éš¾åº¦çº§åˆ«ï¼ˆåˆçº§/ä¸­çº§/é«˜çº§ï¼‰
            
            å†…å®¹ï¼š
            æ ‡é¢˜ï¼š{title}
            æ­£æ–‡ï¼š{full_content}
            '''
            
            result = self._call_llm_tool(question)
            inference_time = time.time() - start_time
            
            if result:
                try:
                    summary_data = json.loads(result)
                    
                    inference_result = InferenceResult(
                        content=cache_key,
                        summary=summary_data.get("summary", ""),
                        sentiment=summary_data.get("sentiment", "ä¸­æ€§"),
                        key_points=summary_data.get("key_points", []),
                        category=summary_data.get("category", "æœªåˆ†ç±»"),
                        difficulty=summary_data.get("difficulty", "ä¸­çº§"),
                        confidence=0.85,
                        inference_time=inference_time
                    )
                    
                    if use_cache:
                        self.cache.set(cache_key, inference_result)
                    
                    return {
                        **summary_data,
                        "confidence": inference_result.confidence,
                        "inference_time": inference_time,
                        "cached": False
                    }
                    
                except json.JSONDecodeError:
                    return self._extract_summary_info(result, title, inference_time)
            
            return {"inference_time": inference_time, "cached": False}
            
        except Exception as e:
            print(f"âŒ å¢å¼ºæ€»ç»“å¼‚å¸¸: {type(e).__name__}: {e}")
            return {}
    
    def _call_llm_tool(self, question: str, timeout: int = 60) -> str:
        """
        è°ƒç”¨LLMå·¥å…·
        
        Args:
            question: é—®é¢˜å†…å®¹
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            LLMå›ç­”
        """
        try:
            args = [
                OCR_CONFIG["python_path"],
                OCR_TOOL,
                "--question",
                question
            ]
            
            result = subprocess.run(
                args,
                shell=False,
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=timeout
            )
            
            if result.returncode != 0:
                print(f"âŒ LLMå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"ğŸ’¬ é”™è¯¯è¾“å‡º: {result.stderr}")
                return ""
            
            output = result.stdout.strip()
            if "=== å¤„ç†ç»“æœ ===" in output:
                result_part = output.split("=== å¤„ç†ç»“æœ ===")[1]
                if "å›ç­”: " in result_part:
                    return result_part.split("å›ç­”: ")[1].strip()
            
            return output
            
        except subprocess.TimeoutExpired:
            print(f"âŒ LLMå·¥å…·è°ƒç”¨è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
            return ""
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLMå·¥å…·å¼‚å¸¸: {type(e).__name__}: {e}")
            return ""
    
    def _extract_summary_info(
        self,
        text: str,
        title: str,
        inference_time: float
    ) -> Dict[str, Any]:
        """
        ä»éJSONæ ¼å¼çš„æ–‡æœ¬ä¸­æå–æ€»ç»“ä¿¡æ¯
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            title: å¸–å­æ ‡é¢˜
            inference_time: æ¨ç†æ—¶é—´
            
        Returns:
            æå–çš„æ€»ç»“ä¿¡æ¯
        """
        return {
            "summary": text[:200] + "..." if len(text) > 200 else text,
            "sentiment": "ä¸­æ€§",
            "key_points": [text[:100] + "..."] if text else [],
            "category": "æœªåˆ†ç±»",
            "difficulty": "ä¸­çº§",
            "confidence": 0.6,
            "inference_time": inference_time,
            "cached": False
        }
    
    def batch_summarize(
        self,
        posts: List[Dict[str, Any]],
        use_cache: bool = True,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æ€»ç»“å¸–å­å†…å®¹
        
        Args:
            posts: å¸–å­åˆ—è¡¨ï¼Œæ¯ä¸ªå¸–å­åŒ…å«titleå’Œcontent
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            æ€»ç»“ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ”§ å¼€å§‹æ‰¹é‡æ€»ç»“ï¼Œå…± {len(posts)} ç¯‡å¸–å­")
        start_time = time.time()
        
        results = []
        
        def process_single(post: Dict[str, Any]) -> Dict[str, Any]:
            title = post.get("title", "") or post.get("basic_info", {}).get("title", "")
            content = post.get("content", "") or post.get("detail", {}).get("desc", "")
            note_id = post.get("note_id", "")
            
            summary = self.summarize_content_enhanced(
                content=content,
                title=title,
                use_cache=use_cache
            )
            
            return {
                "note_id": note_id,
                "title": title,
                "summary": summary,
                "processing_time": time.time()
            }
        
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = [
                executor.submit(process_single, post)
                for post in posts
            ]
            
            for future in futures:
                try:
                    result = future.result(timeout=120)
                    results.append(result)
                    print(f"âœ… å®Œæˆå¸–å­æ€»ç»“: {result['title'][:30]}...")
                except Exception as e:
                    print(f"âŒ æ‰¹é‡æ€»ç»“å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        print(f"âœ… æ‰¹é‡æ€»ç»“å®Œæˆï¼Œ{len(results)} ç¯‡å¸–å­è€—æ—¶ {total_time:.2f} ç§’")
        
        return results
    
    def analyze_image_content(self, image_url: str) -> Dict[str, Any]:
        """
        åˆ†æå›¾åƒå†…å®¹ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œæ ‡ç­¾æå–
        
        Args:
            image_url: å›¾åƒURL
            
        Returns:
            åŒ…å«å›¾åƒåˆ†æç»“æœçš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹å›¾åƒå†…å®¹åˆ†æ: {image_url[:50]}...")
        
        try:
            temp_dir = "/tmp/xhs_image_analysis"
            os.makedirs(temp_dir, exist_ok=True)
            img_save_path = os.path.join(temp_dir, f"image_{int(time.time())}.jpg")
            
            import requests
            response = requests.get(image_url, timeout=30)
            response.raise_for_status()
            with open(img_save_path, 'wb') as f:
                f.write(response.content)
            
            question = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š1. å›¾åƒä¸»è¦å†…å®¹ï¼›2. ç›¸å…³æ ‡ç­¾ï¼ˆ5-10ä¸ªï¼‰ï¼›3. å›¾åƒç±»åˆ«ï¼›4. å…³é”®å…ƒç´ æè¿°"
            
            args = [
                OCR_CONFIG["python_path"],
                OCR_TOOL,
                img_save_path,
                "--question",
                question
            ]
            
            start_time = time.time()
            result = subprocess.run(
                args,
                shell=False,
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=120
            )
            inference_time = time.time() - start_time
            
            if result.returncode != 0:
                print(f"âŒ å›¾åƒåˆ†æå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                return {"error": result.stderr}
            
            output = result.stdout.strip()
            if "=== å¤„ç†ç»“æœ ===" in output:
                result_part = output.split("=== å¤„ç†ç»“æœ ===")[1]
                if "å›ç­”: " in result_part:
                    analysis_text = result_part.split("å›ç­”: ")[1].strip()
                    return {
                        "content": analysis_text,
                        "tags": [analysis_text[:20] for _ in range(5)],
                        "category": "æœªåˆ†ç±»",
                        "elements": [analysis_text[:50]],
                        "inference_time": inference_time
                    }
            
            return {
                "content": output,
                "tags": [],
                "category": "æœªåˆ†ç±»",
                "elements": [],
                "inference_time": inference_time
            }
            
        except Exception as e:
            print(f"âŒ å›¾åƒå†…å®¹åˆ†æå¼‚å¸¸: {type(e).__name__}: {e}")
            return {"error": str(e)}
    
    def build_content_index(self, posts: List[Dict[str, Any]]):
        """
        æ„å»ºå†…å®¹ç´¢å¼•ï¼Œç”¨äºç›¸ä¼¼åº¦æœç´¢
        
        Args:
            posts: å¸–å­åˆ—è¡¨
        """
        print(f"ğŸ”§ å¼€å§‹æ„å»ºå†…å®¹ç´¢å¼•ï¼Œå…± {len(posts)} ç¯‡å¸–å­")
        
        self.posts = posts
        
        post_contents = []
        for post in posts:
            content = ""
            if "basic_info" in post:
                title = post["basic_info"].get("title", "")
                content += title + " "
            if "detail" in post:
                desc = post["detail"].get("desc", "")
                content += desc
            post_contents.append(content)
        
        self.post_vectors = self.tfidf_vectorizer.fit_transform(post_contents)
        print(f"âœ… å†…å®¹ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def search_similar_posts(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹ç›¸ä¼¼åº¦æœç´¢å¸–å­
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼åº¦æ’åºçš„å¸–å­åˆ—è¡¨
        """
        if self.post_vectors is None or self.posts is None:
            print("âŒ å†…å®¹ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_content_index")
            return []
        
        print(f"ğŸ” å¼€å§‹ç›¸ä¼¼åº¦æœç´¢: '{query}'")
        
        query_vector = self.tfidf_vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.post_vectors).flatten()
        
        top_indices = similarities.argsort()[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:
                post = self.posts[idx].copy()
                post["similarity"] = float(similarities[idx])
                results.append(post)
        
        print(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³å¸–å­")
        return results
    
    def recommend_posts(self, post_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹ç›¸ä¼¼åº¦æ¨èå¸–å­
        
        Args:
            post_id: å‚è€ƒå¸–å­ID
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ¨èå¸–å­åˆ—è¡¨
        """
        if self.post_vectors is None or self.posts is None:
            print("âŒ å†…å®¹ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_content_index")
            return []
        
        print(f"ğŸ” å¼€å§‹æ¨èå¸–å­ï¼Œå‚è€ƒID: {post_id}")
        
        ref_idx = -1
        for i, post in enumerate(self.posts):
            if post.get("note_id") == post_id:
                ref_idx = i
                break
        
        if ref_idx == -1:
            print(f"âŒ æœªæ‰¾åˆ°å‚è€ƒå¸–å­: {post_id}")
            return []
        
        ref_vector = self.post_vectors[ref_idx]
        similarities = cosine_similarity(ref_vector, self.post_vectors).flatten()
        
        top_indices = similarities.argsort()[::-1][1:top_k+1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:
                post = self.posts[idx].copy()
                post["similarity"] = float(similarities[idx])
                results.append(post)
        
        print(f"âœ… ç”Ÿæˆ {len(results)} ç¯‡æ¨èå¸–å­")
        return results
    
    def analyze_trends(
        self,
        posts: List[Dict[str, Any]],
        time_window: str = "month"
    ) -> Dict[str, Any]:
        """
        åˆ†æå†…å®¹è¶‹åŠ¿
        
        Args:
            posts: å¸–å­åˆ—è¡¨
            time_window: æ—¶é—´çª—å£ï¼ˆday/week/monthï¼‰
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        print(f"ğŸ“Š å¼€å§‹è¶‹åŠ¿åˆ†æï¼Œå…± {len(posts)} ç¯‡å¸–å­ï¼Œæ—¶é—´çª—å£: {time_window}")
        
        category_counts = {}
        for post in posts:
            category = post.get("category", "æœªåˆ†ç±»")
            category_counts[category] = category_counts.get(category, 0) + 1
        
        all_content = " "
        for post in posts:
            if "title" in post:
                all_content += post["title"] + " "
            if "content" in post:
                all_content += post["content"] + " "
        
        words = re.findall(r'\b\w{2,}\b', all_content)
        word_counts = {}
        stop_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"}
        for word in words:
            if word not in stop_words:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return {
            "category_distribution": category_counts,
            "top_keywords": sorted_words,
            "total_posts": len(posts),
            "time_window": time_window
        }
    
    def clear_cache(self, expired_only: bool = False):
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            expired_only: åªæ¸…é™¤è¿‡æœŸç¼“å­˜
        """
        if expired_only:
            self.cache.clear_expired()
            print("âœ… å·²æ¸…é™¤è¿‡æœŸç¼“å­˜")
        else:
            self.cache = LRUCache(capacity=500, ttl=1800)
            print("âœ… å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "cache_size": len(self.cache.cache),
            "capacity": self.cache.capacity,
            "ttl": self.cache.ttl
        }


def get_ai_utils() -> AIUtils:
    """
    è·å–AIå·¥å…·å®ä¾‹
    
    Returns:
        AIUtilså®ä¾‹
    """
    return AIUtils()


if __name__ == "__main__":
    ai_utils = AIUtils()
    
    print("æµ‹è¯•å¢å¼ºå†…å®¹æ€»ç»“...")
    result = ai_utils.summarize_content_enhanced(
        content="è¿™æ˜¯ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ çš„é¢è¯•ç»éªŒåˆ†äº«...",
        title="æœºå™¨å­¦ä¹ é¢è¯•ç»éªŒæ€»ç»“"
    )
    print(f"æ€»ç»“ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
    
    print("\næµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
    result2 = ai_utils.summarize_content_enhanced(
        content="è¿™æ˜¯ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ çš„é¢è¯•ç»éªŒåˆ†äº«...",
        title="æœºå™¨å­¦ä¹ é¢è¯•ç»éªŒæ€»ç»“"
    )
    print(f"æ˜¯å¦ä½¿ç”¨ç¼“å­˜: {result2.get('cached', False)}")
    
    print("\næµ‹è¯•è¶‹åŠ¿åˆ†æ...")
    trends = ai_utils.analyze_trends([
        {"title": "æµ‹è¯•1", "content": "æœºå™¨å­¦ä¹ å†…å®¹"},
        {"title": "æµ‹è¯•2", "content": "æ·±åº¦å­¦ä¹ å†…å®¹"},
        {"title": "æµ‹è¯•3", "content": "æœºå™¨å­¦ä¹ å†…å®¹"}
    ])
    print(f"è¶‹åŠ¿åˆ†æ: {json.dumps(trends, ensure_ascii=False, indent=2)}")
    
    print("\næµ‹è¯•æ‰¹é‡æ€»ç»“...")
    posts = [
        {"title": f"æµ‹è¯•å¸–å­{i}", "content": f"è¿™æ˜¯ç¬¬{i}ç¯‡æµ‹è¯•å†…å®¹..."}
        for i in range(5)
    ]
    batch_results = ai_utils.batch_summarize(posts)
    print(f"æ‰¹é‡æ€»ç»“å®Œæˆï¼Œå…± {len(batch_results)} ç¯‡")
