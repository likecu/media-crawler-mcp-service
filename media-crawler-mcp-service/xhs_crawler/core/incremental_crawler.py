#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡çˆ¬å–ä¸æ™ºèƒ½å»é‡æ¨¡å—
å®ç°å†…å®¹æŒ‡çº¹ç”Ÿæˆã€ç›¸ä¼¼åº¦æ£€æµ‹ã€å¢é‡çˆ¬å–æ§åˆ¶
"""

import os
import sys
import hashlib
import json
import time
import re
from typing import Dict, Any, Optional, List, Set, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict
import threading

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from xhs_crawler.core.local_database import LocalPostgreSQLDatabase


@dataclass
class ContentFingerprint:
    """å†…å®¹æŒ‡çº¹"""
    note_id: str
    content_hash: str
    title_hash: str
    combined_hash: str
    title: str
    content_preview: str
    created_at: datetime = field(default_factory=datetime.now)
    is_duplicate: bool = False
    similar_to: List[str] = field(default_factory=list)


@dataclass
class DuplicateCheckResult:
    """å»é‡æ£€æµ‹ç»“æœ"""
    is_duplicate: bool
    duplicate_type: str  # exact, similar, new
    duplicate_note_ids: List[str]
    similarity_score: Optional[float]
    fingerprint: ContentFingerprint


class IncrementalCrawler:
    """
    å¢é‡çˆ¬å–æ§åˆ¶å™¨
    ç®¡ç†çˆ¬å–çŠ¶æ€ï¼Œå®ç°å¢é‡æ›´æ–°å’Œæ™ºèƒ½å»é‡
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–å¢é‡çˆ¬å–æ§åˆ¶å™¨"""
        if self._initialized:
            return
        
        self.db = None
        self.content_fingerprints: Dict[str, ContentFingerprint] = {}
        self.tfidf_vectorizer = TfidfVectorizer(
            stop_words=None,
            max_features=1000,
            ngram_range=(1, 2)
        )
        self.content_vectors = None
        self.similarity_threshold = 0.85
        self._initialized = True
        self._lock = threading.Lock()
    
    def _get_database(self) -> LocalPostgreSQLDatabase:
        """è·å–æ•°æ®åº“è¿æ¥"""
        if self.db is None:
            self.db = LocalPostgreSQLDatabase()
        return self.db
    
    def _generate_content_hash(self, content: str) -> str:
        """
        ç”Ÿæˆå†…å®¹å“ˆå¸Œ
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            å†…å®¹å“ˆå¸Œå€¼ï¼ˆMD5ï¼‰
        """
        normalized = re.sub(r'\s+', ' ', content.strip())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _generate_title_hash(self, title: str) -> str:
        """
        ç”Ÿæˆæ ‡é¢˜å“ˆå¸Œ
        
        Args:
            title: æ ‡é¢˜
            
        Returns:
            æ ‡é¢˜å“ˆå¸Œå€¼ï¼ˆMD5ï¼‰
        """
        normalized = re.sub(r'[^\w\u4e00-\u9fff]', '', title.strip())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _generate_combined_hash(self, title: str, content: str) -> str:
        """
        ç”Ÿæˆæ ‡é¢˜+å†…å®¹ç»„åˆå“ˆå¸Œ
        
        Args:
            title: æ ‡é¢˜
            content: å†…å®¹
            
        Returns:
            ç»„åˆå“ˆå¸Œå€¼ï¼ˆMD5ï¼‰
        """
        combined = f"{title[:100]}|{content[:500]}".encode('utf-8')
        return hashlib.md5(combined).hexdigest()
    
    def _create_fingerprint(
        self,
        note_id: str,
        title: str,
        content: str
    ) -> ContentFingerprint:
        """
        åˆ›å»ºå†…å®¹æŒ‡çº¹
        
        Args:
            note_id: ç¬”è®°ID
            title: æ ‡é¢˜
            content: å†…å®¹
            
        Returns:
            å†…å®¹æŒ‡çº¹
        """
        return ContentFingerprint(
            note_id=note_id,
            content_hash=self._generate_content_hash(content),
            title_hash=self._generate_title_hash(title),
            combined_hash=self._generate_combined_hash(title, content),
            title=title[:100],
            content_preview=content[:200] if content else ""
        )
    
    def check_duplicate(
        self,
        note_id: str,
        title: str,
        content: str,
        check_exact: bool = True,
        check_similar: bool = True
    ) -> DuplicateCheckResult:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦é‡å¤
        
        Args:
            note_id: ç¬”è®°ID
            title: æ ‡é¢˜
            content: å†…å®¹
            check_exact: æ˜¯å¦æ£€æµ‹å®Œå…¨é‡å¤
            check_similar: æ˜¯å¦æ£€æµ‹ç›¸ä¼¼å†…å®¹
            
        Returns:
            å»é‡æ£€æµ‹ç»“æœ
        """
        with self._lock:
            new_fingerprint = self._create_fingerprint(note_id, title, content)
            duplicate_note_ids = []
            similarity_score = None
            
            if check_exact:
                for fp in self.content_fingerprints.values():
                    if fp.combined_hash == new_fingerprint.combined_hash:
                        duplicate_note_ids.append(fp.note_id)
                        new_fingerprint.is_duplicate = True
                        return DuplicateCheckResult(
                            is_duplicate=True,
                            duplicate_type="exact",
                            duplicate_note_ids=duplicate_note_ids,
                            similarity_score=1.0,
                            fingerprint=new_fingerprint
                        )
                
                for fp in self.content_fingerprints.values():
                    if fp.content_hash == new_fingerprint.content_hash:
                        duplicate_note_ids.append(fp.note_id)
                        new_fingerprint.is_duplicate = True
                        return DuplicateCheckResult(
                            is_duplicate=True,
                            duplicate_type="exact",
                            duplicate_note_ids=duplicate_note_ids,
                            similarity_score=1.0,
                            fingerprint=new_fingerprint
                        )
            
            if check_similar and self.content_vectors is not None:
                new_vector = self.tfidf_vectorizer.transform([title + " " + content])
                similarities = cosine_similarity(new_vector, self.content_vectors)[0]
                max_similarity = float(np.max(similarities))
                most_similar_idx = int(np.argmax(similarities))
                
                if max_similarity >= self.similarity_threshold:
                    similar_note_ids = [
                        list(self.content_fingerprints.keys())[i]
                        for i in range(len(similarities))
                        if similarities[i] >= self.similarity_threshold
                    ]
                    duplicate_note_ids.extend(similar_note_ids)
                    new_fingerprint.is_duplicate = True
                    new_fingerprint.similar_to = similar_note_ids
                    return DuplicateCheckResult(
                        is_duplicate=True,
                        duplicate_type="similar",
                        duplicate_note_ids=list(set(duplicate_note_ids)),
                        similarity_score=max_similarity,
                        fingerprint=new_fingerprint
                    )
                
                similarity_score = max_similarity
            
            return DuplicateCheckResult(
                is_duplicate=False,
                duplicate_type="new",
                duplicate_note_ids=duplicate_note_ids,
                similarity_score=similarity_score,
                fingerprint=new_fingerprint
            )
    
    def add_content(
        self,
        note_id: str,
        title: str,
        content: str
    ) -> ContentFingerprint:
        """
        æ·»åŠ æ–°å†…å®¹åˆ°æŒ‡çº¹åº“
        
        Args:
            note_id: ç¬”è®°ID
            title: æ ‡é¢˜
            content: å†…å®¹
            
        Returns:
            åˆ›å»ºçš„å†…å®¹æŒ‡çº¹
        """
        with self._lock:
            fingerprint = self._create_fingerprint(note_id, title, content)
            self.content_fingerprints[note_id] = fingerprint
            return fingerprint
    
    def load_existing_fingerprints(
        self,
        source: str = None,
        days: int = 30
    ) -> int:
        """
        ä»æ•°æ®åº“åŠ è½½ç°æœ‰å†…å®¹æŒ‡çº¹
        
        Args:
            source: æ•°æ®æ¥æºï¼ˆå¯é€‰ï¼‰
            days: åŠ è½½æœ€è¿‘å‡ å¤©çš„æ•°æ®
            
        Returns:
            åŠ è½½çš„æŒ‡çº¹æ•°é‡
        """
        try:
            db = self._get_database()
            
            query = """
            SELECT note_id, title, content, created_at
            FROM leetcode_practice
            WHERE created_at >= %s
            """
            cutoff_date = datetime.now() - timedelta(days=days)
            params = [cutoff_date]
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, params)
                rows = cursor.fetchall()
            
            for row in rows:
                note_id, title, content, created_at = row
                fingerprint = self._create_fingerprint(note_id, title, content or "")
                fingerprint.created_at = created_at
                self.content_fingerprints[note_id] = fingerprint
            
            self._rebuild_tfidf_vectors()
            
            print(f"âœ… åŠ è½½äº† {len(self.content_fingerprints)} ä¸ªå†…å®¹æŒ‡çº¹")
            return len(self.content_fingerprints)
            
        except Exception as e:
            print(f"âŒ åŠ è½½å†…å®¹æŒ‡çº¹å¤±è´¥: {e}")
            return 0
    
    def save_fingerprint(
        self,
        fingerprint: ContentFingerprint,
        source: str = "default"
    ) -> bool:
        """
        ä¿å­˜å†…å®¹æŒ‡çº¹åˆ°æ•°æ®åº“
        
        Args:
            fingerprint: å†…å®¹æŒ‡çº¹å¯¹è±¡
            source: æ•°æ®æ¥æºæ ‡è¯†
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            db = self._get_database()
            
            query = """
            INSERT INTO content_fingerprints 
            (note_id, title, content_hash, title_hash, combined_hash, 
             title_preview, content_preview, source, created_at, is_duplicate, similar_to)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (note_id) DO UPDATE SET
                title = EXCLUDED.title,
                content_hash = EXCLUDED.content_hash,
                title_hash = EXCLUDED.title_hash,
                combined_hash = EXCLUDED.combined_hash,
                title_preview = EXCLUDED.title_preview,
                content_preview = EXCLUDED.content_preview,
                source = EXCLUDED.source,
                created_at = EXCLUDED.created_at,
                is_duplicate = EXCLUDED.is_duplicate,
                similar_to = EXCLUDED.similar_to,
                updated_at = CURRENT_TIMESTAMP
            """
            
            params = [
                fingerprint.note_id,
                fingerprint.title,
                fingerprint.content_hash,
                fingerprint.title_hash,
                fingerprint.combined_hash,
                fingerprint.title[:200] if fingerprint.title else "",
                fingerprint.content_preview,
                source,
                fingerprint.created_at,
                fingerprint.is_duplicate,
                json.dumps(fingerprint.similar_to) if fingerprint.similar_to else None
            ]
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, params)
                conn.commit()
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å†…å®¹æŒ‡çº¹å¤±è´¥: {e}")
            return False
    
    def save_fingerprints_batch(
        self,
        fingerprints: List[ContentFingerprint],
        source: str = "default"
    ) -> int:
        """
        æ‰¹é‡ä¿å­˜å†…å®¹æŒ‡çº¹åˆ°æ•°æ®åº“
        
        Args:
            fingerprints: å†…å®¹æŒ‡çº¹åˆ—è¡¨
            source: æ•°æ®æ¥æºæ ‡è¯†
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not fingerprints:
            return 0
        
        try:
            db = self._get_database()
            
            query = """
            INSERT INTO content_fingerprints 
            (note_id, title, content_hash, title_hash, combined_hash, 
             title_preview, content_preview, source, created_at, is_duplicate, similar_to)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (note_id) DO UPDATE SET
                title = EXCLUDED.title,
                content_hash = EXCLUDED.content_hash,
                title_hash = EXCLUDED.title_hash,
                combined_hash = EXCLUDED.combined_hash,
                title_preview = EXCLUDED.title_preview,
                content_preview = EXCLUDED.content_preview,
                source = EXCLUDED.source,
                is_duplicate = EXCLUDED.is_duplicate,
                similar_to = EXCLUDED.similar_to,
                updated_at = CURRENT_TIMESTAMP
            """
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                for fingerprint in fingerprints:
                    params = [
                        fingerprint.note_id,
                        fingerprint.title,
                        fingerprint.content_hash,
                        fingerprint.title_hash,
                        fingerprint.combined_hash,
                        fingerprint.title[:200] if fingerprint.title else "",
                        fingerprint.content_preview,
                        source,
                        fingerprint.created_at,
                        fingerprint.is_duplicate,
                        json.dumps(fingerprint.similar_to) if fingerprint.similar_to else None
                    ]
                    cursor.execute(query, params)
                conn.commit()
            
            saved_count = len(fingerprints)
            print(f"âœ… æ‰¹é‡ä¿å­˜äº† {saved_count} ä¸ªå†…å®¹æŒ‡çº¹")
            return saved_count
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ä¿å­˜å†…å®¹æŒ‡çº¹å¤±è´¥: {e}")
            try:
                with db.get_connection() as conn:
                    conn.rollback()
            except:
                pass
            return 0
    
    def mark_duplicate_in_db(
        self,
        note_id: str,
        duplicate_of: List[str],
        duplicate_type: str = "similar"
    ) -> bool:
        """
        åœ¨æ•°æ®åº“ä¸­æ ‡è®°å†…å®¹ä¸ºé‡å¤
        
        Args:
            note_id: ç¬”è®°ID
            duplicate_of: é‡å¤å†…å®¹çš„IDåˆ—è¡¨
            duplicate_type: é‡å¤ç±»å‹
            
        Returns:
            æ˜¯å¦æ ‡è®°æˆåŠŸ
        """
        try:
            db = self._get_database()
            
            query = """
            UPDATE content_fingerprints 
            SET is_duplicate = TRUE, 
                duplicate_type = %s,
                similar_to = %s,
                updated_at = CURRENT_TIMESTAMP
            WHERE note_id = %s
            """
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, (duplicate_type, json.dumps(duplicate_of), note_id))
                conn.commit()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ ‡è®°é‡å¤å†…å®¹å¤±è´¥: {e}")
            return False
    
    def get_duplicate_stats(self, source: str = None) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åº“ä¸­çš„é‡å¤ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            source: æ•°æ®æ¥æºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            db = self._get_database()
            
            query = """
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN is_duplicate THEN 1 ELSE 0 END) as duplicates,
                COUNT(DISTINCT source) as source_count
            FROM content_fingerprints
            """
            params = []
            
            if source:
                query += " WHERE source = %s"
                params.append(source)
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, params)
                row = cursor.fetchone()
            
            return {
                "total_fingerprints": row[0] or 0,
                "duplicate_count": row[1] or 0,
                "unique_count": (row[0] or 0) - (row[1] or 0),
                "source_count": row[2] or 0
            }
            
        except Exception as e:
            print(f"âŒ è·å–é‡å¤ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_fingerprints": 0, "duplicate_count": 0, "unique_count": 0, "source_count": 0}
    
    def cleanup_old_fingerprints(self, days: int = 90) -> int:
        """
        æ¸…ç†è¿‡æœŸå†…å®¹æŒ‡çº¹
        
        Args:
            days: ä¿ç•™æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
            
        Returns:
            åˆ é™¤çš„æŒ‡çº¹æ•°é‡
        """
        try:
            db = self._get_database()
            
            cutoff_date = datetime.now() - timedelta(days=days)
            
            query = """
            DELETE FROM content_fingerprints 
            WHERE created_at < %s AND is_duplicate = TRUE
            """
            
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, [cutoff_date])
                deleted_count = cursor.rowcount
                conn.commit()
            
            print(f"ğŸ—‘ï¸ æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸæŒ‡çº¹")
            return deleted_count
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡æœŸæŒ‡çº¹å¤±è´¥: {e}")
            return 0
    
    def _rebuild_tfidf_vectors(self):
        """é‡å»ºTF-IDFå‘é‡"""
        if len(self.content_fingerprints) == 0:
            self.content_vectors = None
            return
        
        texts = [
            fp.title + " " + fp.content_preview
            for fp in self.content_fingerprints.values()
        ]
        
        if texts:
            self.content_vectors = self.tfidf_vectorizer.fit_transform(texts)
            print(f"âœ… é‡å»ºTF-IDFå‘é‡ï¼Œå®Œæˆ {len(texts)} ä¸ªæ–‡æœ¬")
    
    def set_similarity_threshold(self, threshold: float):
        """
        è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Args:
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        """
        self.similarity_threshold = max(0.0, min(1.0, threshold))
        print(f"ğŸ“Š ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®ä¸º: {self.similarity_threshold}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = len(self.content_fingerprints)
        duplicates = sum(1 for fp in self.content_fingerprints.values() if fp.is_duplicate)
        
        return {
            "total_fingerprints": total,
            "duplicate_count": duplicates,
            "unique_count": total - duplicates,
            "similarity_threshold": self.similarity_threshold,
            "vector_dimensions": self.content_vectors.shape[1] if self.content_vectors is not None else 0
        }
    
    def get_source_statistics(self) -> Dict[str, Dict[str, int]]:
        """
        è·å–æŒ‰æ¥æºåˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            æ¥æºç»Ÿè®¡å­—å…¸ {source: {"total": x, "duplicates": y}}
        """
        source_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {"total": 0, "duplicates": 0})
        
        for fp in self.content_fingerprints.values():
            source = getattr(fp, 'source', 'unknown') if hasattr(fp, 'source') else 'unknown'
            source_stats[source]["total"] += 1
            if fp.is_duplicate:
                source_stats[source]["duplicates"] += 1
        
        return dict(source_stats)
    
    def get_duplicate_rate(self) -> float:
        """
        è®¡ç®—é‡å¤ç‡
        
        Returns:
            é‡å¤ç‡ï¼ˆ0-1ï¼‰
        """
        total = len(self.content_fingerprints)
        if total == 0:
            return 0.0
        duplicates = sum(1 for fp in self.content_fingerprints.values() if fp.is_duplicate)
        return duplicates / total
    
    def generate_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆå»é‡ç»Ÿè®¡æŠ¥å‘Š
        
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        stats = self.get_statistics()
        source_stats = self.get_source_statistics()
        
        db_stats = self.get_duplicate_stats()
        
        return {
            "timestamp": datetime.now().isoformat(),
            "memory_stats": stats,
            "database_stats": db_stats,
            "source_statistics": source_stats,
            "duplicate_rate": self.get_duplicate_rate(),
            "similarity_threshold": self.similarity_threshold
        }
    
    def print_report(self):
        """æ‰“å°å»é‡ç»Ÿè®¡æŠ¥å‘Š"""
        report = self.generate_report()
        
        print("\n" + "="*60)
        print("ğŸ“Š å¢é‡çˆ¬å–å»é‡ç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        print(f"ç”Ÿæˆæ—¶é—´: {report['timestamp']}")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {report['similarity_threshold']}")
        print("-"*60)
        print("å†…å­˜ä¸­çš„æŒ‡çº¹ç»Ÿè®¡:")
        print(f"  æ€»æ•°é‡: {report['memory_stats']['total_fingerprints']}")
        print(f"  é‡å¤æ•°: {report['memory_stats']['duplicate_count']}")
        print(f"  å”¯ä¸€æ•°: {report['memory_stats']['unique_count']}")
        print(f"  é‡å¤ç‡: {report['duplicate_rate']:.2%}")
        print("-"*60)
        print("æ•°æ®åº“ä¸­çš„æŒ‡çº¹ç»Ÿè®¡:")
        print(f"  æ€»æ•°é‡: {report['database_stats']['total_fingerprints']}")
        print(f"  é‡å¤æ•°: {report['database_stats']['duplicate_count']}")
        print(f"  å”¯ä¸€æ•°: {report['database_stats']['unique_count']}")
        print(f"  æ¥æºæ•°: {report['database_stats']['source_count']}")
        print("-"*60)
        print("æŒ‰æ¥æºç»Ÿè®¡:")
        for source, s_stats in report['source_statistics'].items():
            rate = s_stats["duplicates"] / s_stats["total"] if s_stats["total"] > 0 else 0
            print(f"  {source}: {s_stats['total']} æ¡, {s_stats['duplicates']} é‡å¤ ({rate:.2%})")
        print("="*60 + "\n")
    
    def clear_fingerprints(self):
        """æ¸…ç©ºæ‰€æœ‰æŒ‡çº¹æ•°æ®"""
        with self._lock:
            self.content_fingerprints.clear()
            self.content_vectors = None
            print("ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰å†…å®¹æŒ‡çº¹")


def get_incremental_crawler() -> IncrementalCrawler:
    """
    è·å–å¢é‡çˆ¬å–æ§åˆ¶å™¨å®ä¾‹
    
    Returns:
        IncrementalCrawler å®ä¾‹
    """
    return IncrementalCrawler()


class DuplicateChecker:
    """
    æ‰¹é‡å†…å®¹å»é‡æ£€æŸ¥å™¨
    æ”¯æŒæ‰¹é‡æ£€æµ‹å’Œå¹¶è¡Œå¤„ç†
    """
    
    def __init__(
        self,
        similarity_threshold: float = 0.85,
        max_workers: int = 4
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å»é‡æ£€æŸ¥å™¨
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_workers: æœ€å¤§å¹¶å‘æ•°
        """
        self.incremental_crawler = get_incremental_crawler()
        self.similarity_threshold = similarity_threshold
        self.max_workers = max_workers
        self.incremental_crawler.set_similarity_threshold(similarity_threshold)
    
    def check_batch(
        self,
        items: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        æ‰¹é‡æ£€æµ‹å†…å®¹é‡å¤
        
        Args:
            items: å†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« note_id, title, content
            
        Returns:
            (æ–°å†…å®¹åˆ—è¡¨, é‡å¤å†…å®¹åˆ—è¡¨)
        """
        new_items = []
        duplicate_items = []
        
        for item in items:
            note_id = item.get("note_id", "")
            title = item.get("title", "")
            content = item.get("content", "")
            
            result = self.incremental_crawler.check_duplicate(
                note_id=note_id,
                title=title,
                content=content,
                check_exact=True,
                check_similar=True
            )
            
            item_result = {
                **item,
                "duplicate_type": result.duplicate_type,
                "similarity_score": result.similarity_score,
                "duplicate_note_ids": result.duplicate_note_ids
            }
            
            if result.is_duplicate:
                duplicate_items.append(item_result)
            else:
                new_items.append(item_result)
                self.incremental_crawler.add_content(note_id, title, content)
        
        return new_items, duplicate_items
    
    def filter_new_items(
        self,
        items: List[Dict[str, str]]
    ) -> List[Dict[str, str]]:
        """
        è¿‡æ»¤å‡ºæ–°å¢å†…å®¹ï¼ˆå»é‡ï¼‰
        
        Args:
            items: åŸå§‹å†…å®¹åˆ—è¡¨
            
        Returns:
            ä»…åŒ…å«æ–°å¢å†…å®¹çš„åˆ—è¡¨
        """
        new_items, _ = self.check_batch(items)
        return new_items


def filter_duplicate_posts(
    posts: List[Dict[str, Any]],
    threshold: float = 0.85
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    è¿‡æ»¤é‡å¤å¸–å­
    
    Args:
        posts: å¸–å­åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        (æ–°å¸–å­åˆ—è¡¨, é‡å¤å¸–å­åˆ—è¡¨)
    """
    checker = DuplicateChecker(similarity_threshold=threshold)
    
    items = [
        {
            "note_id": post.get("note_id", post.get("id", "")),
            "title": post.get("title", "") or post.get("basic_info", {}).get("title", ""),
            "content": post.get("content", "") or post.get("detail", {}).get("desc", "")
        }
        for post in posts
    ]
    
    new_items, duplicate_items = checker.check_batch(items)
    
    new_posts = [
        post for post in posts
        if post.get("note_id", post.get("id", "")) in [i["note_id"] for i in new_items]
    ]
    
    duplicate_posts = [
        post for post in posts
        if post.get("note_id", post.get("id", "")) in [i["note_id"] for i in duplicate_items]
    ]
    
    return new_posts, duplicate_posts
