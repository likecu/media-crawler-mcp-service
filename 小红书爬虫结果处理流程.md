# 小红书爬虫结果处理流程文档

## 一、概述

本文档描述了小红书爬虫从触发到结果处理的完整流程，包括数据采集、内容增强、清洗存储等环节。

## 二、系统架构

### 2.1 核心组件

```
┌─────────────────────────────────────────────────────────────────┐
│                      小红书爬虫系统                              │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │
│  │  入口程序   │→ │  爬虫核心   │→ │  MCP工具调用层          │  │
│  │ example.py  │  │ BaseCrawler │  │ MCPUtils                │  │
│  └─────────────┘  └─────────────┘  └─────────────────────────┘  │
│         │                │                     │                │
│         ▼                ▼                     ▼                │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              MCP Server (xhs_search等工具)               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              │                                  │
│         ┌────────────────────┼────────────────────┐            │
│         ▼                    ▼                    ▼            │
│  ┌─────────────┐    ┌─────────────────┐    ┌─────────────┐    │
│  │ 数据清洗    │    │  AI内容增强     │    │ 数据库存储  │    │
│  │ JsonCleaner │    │  AIUtils        │    │ NeonDatabase│    │
│  └─────────────┘    └─────────────────┘    └─────────────┘    │
│         │                    │                    │            │
│         └────────────────────┴────────────────────┘            │
│                            │                                    │
│                            ▼                                    │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              HTML生成 & 上传到数据库                     │   │
│  │              generate_html()                            │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 模块职责

| 模块 | 文件路径 | 职责 |
|------|----------|------|
| 爬虫基类 | [base_crawler.py](xhs_crawler/core/base_crawler.py) | 封装通用的搜索、详情获取、评论获取方法 |
| 简单爬虫 | [simple_xhs_crawler.py](xhs_crawler/crawlers/simple_xhs_crawler.py) | 单关键词简单爬取 |
| 多关键词爬虫 | [multi_keyword_crawler.py](xhs_crawler/crawlers/multi_keyword_crawler.py) | 多关键词批量爬取 |
| MCP工具 | [mcp_utils.py](xhs_crawler/core/mcp_utils.py) | 调用MCP服务器执行爬取 |
| AI增强 | [ai_utils.py](xhs_crawler/core/ai_utils.py) | 内容总结、相似度计算、趋势分析 |
| 数据清洗 | [clean_json_files.py](xhs_crawler/cleaners/clean_json_files.py) | 敏感字段清理 |
| 数据库 | [database.py](xhs_crawler/core/database.py) | Neon PostgreSQL存储 |
| HTML生成 | [html_generator.py](xhs_crawler/generators/html_generator.py) | 生成可浏览的HTML页面 |

## 三、处理流程详解

### 3.1 流程总览

```
触发爬虫
    │
    ▼
┌─────────────────┐
│ 1. 读取配置     │──→ search_config.json
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 2. 搜索帖子     │──→ call_mcp_tool("xhs_search")
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 3. 获取详情     │──→ call_mcp_tool("xhs_crawler_detail")
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 4. 数据清洗     │──→ JsonCleaner.clean_json_files_in_directory()
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 5. AI内容增强   │──→ AIUtils.summarize_content_enhanced()
│                 │   ├── 内容总结
│                 │   ├── 情感分析
│                 │   ├── 关键信息提取
│                 │   └── 图像OCR分析
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 6. 构建索引     │──→ AIUtils.build_content_index()
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 7. HTML生成     │──→ generate_html()
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 8. 数据库存储   │──→ NeonDatabase.upload_content()
└─────────────────┘
```

### 3.2 步骤详解

#### 步骤1：读取配置

**入口**：[example.py](example.py) → `run_multi_keyword_crawler()`

**配置来源**：`search_config.json`

```python
# search_config.json 结构
{
    "search_terms": [
        "大模型面试 经验分享",
        "大模型面试技巧",
        "大模型面试题",
        ...  # 更多关键词
    ],
    "page_num": 2,       # 每关键词爬取页数
    "page_size": 10,     # 每页帖子数量
    "max_threads": 4     # 并行处理线程数
}
```

**相关代码**：
```python
# 从配置文件读取参数
with open("search_config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

keywords = config.get("search_terms", ["大模型面试"])
max_pages = config.get("page_num", 2)
page_size = config.get("page_size", 10)
```

#### 步骤2：搜索帖子

**方法**：`BaseCrawler.search_posts()`

**调用MCP工具**：`xhs_search`

```python
def search_posts(self, keywords: str, page_num: int, page_size: int) -> List[Dict[str, Any]]:
    """
    搜索小红书帖子
    
    Args:
        keywords: 搜索关键词
        page_num: 页码
        page_size: 每页数量
        
    Returns:
        帖子列表（包含 note_id, xsec_token, title, user, interact_info 等）
    """
    result = self.mcp_utils.call_mcp_tool("xhs_search", {
        "keywords": keywords,
        "page_num": page_num,
        "page_size": page_size
    })
    # 返回搜索结果
```

**返回数据结构**：
```python
{
    "note_id": "xxx",
    "xsec_token": "xxx",
    "xsec_source": "pc_feed",
    "title": "帖子标题",
    "note_url": "https://www.xiaohongshu.com/explore/xxx",
    "user": {
        "nickname": "用户名",
        "avatar": "头像URL"
    },
    "interact_info": {
        "liked_count": 点赞数,
        "collected_count": 收藏数,
        "comment_count": 评论数,
        "share_count": 分享数
    }
}
```

#### 步骤3：获取帖子详情

**方法**：`BaseCrawler.get_post_detail()`

**调用MCP工具**：`xhs_crawler_detail`

```python
def get_post_detail(self, note_id: str, xsec_token: str, xsec_source: str) -> Dict[str, Any]:
    """
    获取帖子详情
    
    Args:
        note_id: 帖子ID
        xsec_token: 安全令牌
        xsec_source: 来源
        
    Returns:
        帖子详情（包含 desc, imageList, time, location 等）
    """
    result = self.mcp_utils.call_mcp_tool("xhs_crawler_detail", {
        "note_id": note_id,
        "xsec_token": xsec_token,
        "xsec_source": xsec_source
    })
```

**返回数据结构**：
```python
{
    "desc": "帖子正文内容",
    "imageList": [
        {"url": "图片1URL", "type": "image"},
        {"url": "图片2URL", "type": "image"}
    ],
    "time": "发布时间",
    "last_update_time": "最后更新时间"
}
```

#### 步骤4：数据清洗

**类**：`JsonCleaner`

**功能**：递归删除JSON中的敏感字段

**默认清理字段**：
```python
DEFAULT_SENSITIVE_FIELDS = [
    "xsec_token", "xsec_source", "cookie", "token",
    "access_token", "refresh_token", "secret", "api_key",
    "password", "passwd", "auth", "authorization",
    "session", "session_id", "session_key", "csrf_token",
    "uid", "user_id", "user_token", "secret_key",
    "private_key", "public_key", "key_id", "app_id", "app_secret"
]
```

**使用方式**：
```python
cleaner = JsonCleaner(fields_to_remove)
cleaner.clean_json_files_in_directory(output_dir)
cleaner.print_statistics()
```

**输出统计**：
```
清理统计信息：
总文件数: 50
已清理文件数: 30
删除的敏感字段总数: 150
使用的敏感字段列表长度: 25
```

#### 步骤5：AI内容增强

**类**：`AIUtils`

**功能**：
1. **增强内容总结**：`summarize_content_enhanced()`
2. **图像内容分析**：`analyze_image_content()`
3. **情感分析**：判断内容情感倾向（积极/中性/消极）
4. **关键信息提取**：自动提取5-10个关键要点

**调用LLM工具**：
```python
def _call_llm_tool(self, question: str) -> str:
    """
    调用LLM工具（gemini_ocr.py）
    """
    args = [
        "/Users/aaa/python-sdk/python3.13.2/bin/python",
        "/Volumes/600g/app1/doubao获取/python/gemini_ocr.py",
        "--question", question
    ]
    result = subprocess.run(args, capture_output=True, text=True)
    return result.stdout
```

**返回数据结构**：
```python
{
    "summary": "主要内容总结（200字以内）",
    "sentiment": "积极/中性/消极",
    "key_points": ["要点1", "要点2", ...],
    "category": "内容类别",
    "difficulty": "初级/中级/高级"
}
```

**图片OCR处理**：
```python
def summarize_posts():
    """
    对帖子内容进行总结，包括图片OCR
    """
    # 1. 加载帖子详情
    posts = load_post_details()
    
    # 2. 提取图片URL
    images = detail.get("imageList", [])
    
    # 3. 并行下载图片
    for img_url in images:
        download_image(img_url, temp_path)
    
    # 4. 调用gemini_ocr.py进行OCR
    ocr_result = ocr_image(image_path)
    
    # 5. 将OCR结果融入内容总结
    full_content = original_content + "\n\n--- 图片OCR结果 ---\n" + ocr_result
```

#### 步骤6：构建内容索引

**方法**：`AIUtils.build_content_index()`

**功能**：使用TF-IDF构建内容相似度索引

```python
def build_content_index(self, posts: List[Dict[str, Any]]):
    """
    构建TF-IDF内容索引
    
    Args:
        posts: 帖子列表
    """
    # 1. 提取帖子内容
    post_contents = []
    for post in posts:
        content = post["basic_info"].get("title", "")
        content += " " + post["detail"].get("desc", "")
        post_contents.append(content)
    
    # 2. 构建TF-IDF向量
    self.post_vectors = self.tfidf_vectorizer.fit_transform(post_contents)
```

**后续功能**：
```python
# 相似度搜索
similar_posts = ai_utils.search_similar_posts("面试技巧", top_k=5)

# 推荐相关帖子
recommendations = ai_utils.recommend_posts(note_id="xxx", top_k=5)

# 趋势分析
trends = ai_utils.analyze_trends(posts, time_window="month")
```

#### 步骤7：HTML生成

**函数**：`generate_html()`

**位置**：[html_generator.py](xhs_crawler/generators/html_generator.py)

```python
def generate_html(posts: List[Dict[str, Any]], html_file: str, title: str) -> bool:
    """
    生成HTML网页并上传到Neon数据库
    
    Args:
        posts: 帖子列表
        html_file: HTML文件名
        title: 网页标题
        
    Returns:
        是否生成成功
    """
    # 1. 生成帖子HTML
    posts_html = ""
    for i, post in enumerate(posts):
        posts_html += generate_post_html(post, i)
    
    # 2. 构建完整HTML
    html_content = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <title>{title}</title>
        <style>
            body {{ font-family: -apple-system, sans-serif; background: #f5f5f5; }}
            .container {{ max-width: 1200px; margin: 0 auto; }}
            .post {{ background: white; margin-bottom: 30px; padding: 20px; border-radius: 8px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>{title}</h1>
            {posts_html}
        </div>
    </body>
    </html>
    """
    
    # 3. 上传到数据库
    db = get_neon_database()
    db.upload_content(filename, html_content, "html", hashid)
```

#### 步骤8：数据库存储

**类**：`NeonDatabase`

**位置**：[database.py](xhs_crawler/core/database.py)

**支持的表结构**：

1. **files表** - 通用文件存储
```sql
CREATE TABLE files (
    id SERIAL PRIMARY KEY,
    filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(50) NOT NULL,
    file_content BYTEA NOT NULL,
    hashid VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(filename),
    UNIQUE(hashid)
);
```

2. **interview_questions表** - 面试题库
```sql
CREATE TABLE interview_questions (
    id SERIAL PRIMARY KEY,
    question_id VARCHAR(50) NOT NULL UNIQUE,
    content TEXT NOT NULL,
    answer TEXT,
    category VARCHAR(100),
    difficulty VARCHAR(20),
    source VARCHAR(500),
    note_id VARCHAR(100)
);
```

3. **practice_records表** - 刷题记录
```sql
CREATE TABLE practice_records (
    id SERIAL PRIMARY KEY,
    keyword VARCHAR(200) NOT NULL,
    platform VARCHAR(50),
    note_count INTEGER DEFAULT 0
);
```

**核心方法**：
```python
# 上传文件
db.upload_file(file_path, hashid)

# 上传内容（直接上传字符串）
db.upload_content(filename, content, file_type="html", hashid)

# 上传目录中的所有文件
db.upload_files_in_directory(directory_path, extensions=["html", "txt"])

# 获取文件
db.get_file(filename)
db.get_file_by_hashid(hashid)

# 下载文件
db.download_file(filename, output_path)
```

## 四、输出目录结构

```
大模型面试帖子/                    # 输出目录
├── 001_xxx_title.json            # 原始搜索结果
├── 002_xxx_title.json
├── ...
├── 详情/                          # 帖子详情目录
│   ├── 001_xxx_title_detail.json  # 包含basic_info + detail
│   ├── 002_xxx_title_detail.json
│   └── ...
├── 总结/                          # AI总结目录
│   ├── xxx_title_summary.txt
│   └── ocr_cache.json            # OCR结果缓存
├── 大模型面试经验分享.html         # HTML页面
└── 大模型面试经验分享_with_summary.html  # 带AI总结的HTML
```

## 五、关键配置项

### 5.1 搜索配置 (search_config.json)

```json
{
    "search_terms": [
        "大模型面试 经验分享",
        "大模型面试技巧",
        "大模型面试题"
    ],
    "page_num": 2,
    "page_size": 10,
    "max_threads": 4
}
```

### 5.2 爬虫配置 (config.py)

```python
DEFAULT_CRAWLER_CONFIG = {
    "max_pages": 3,           # 最大爬取页数
    "page_size": 30,          # 每页结果数
    "sleep_time": 3           # 请求间隔（秒）
}

OUTPUT_DIR_CONFIG = {
    "simple": "大模型面试帖子",
    "multi_keyword": "大模型面试帖子_all",
    "interview": "大模型面试帖子"
}
```

### 5.3 OCR配置

```python
OCR_CONFIG = {
    "python_path": "/Users/aaa/python-sdk/python3.13.2/bin/python",
    "tool_path": "/Volumes/600g/app1/doubao获取/python/gemini_ocr.py",
    "question": "图里有什么内容？",
    "max_threads": 4
}
```

## 六、异常处理

### 6.1 MCP工具调用失败

```python
def call_mcp_tool(self, tool_name: str, params: Dict) -> Dict:
    try:
        response = requests.post(
            "http://localhost:9091/api/admin/inspector/execute",
            json={"tool": tool_name, "params": params},
            timeout=30
        )
        return response.json().get("result", {})
    except requests.exceptions.RequestException as e:
        print(f"❌ MCP工具调用失败: {tool_name}, {e}")
        return {"code": -1, "msg": str(e)}
```

### 6.2 OCR超时处理

```python
result = subprocess.run(
    args,
    capture_output=True,
    text=True,
    timeout=120  # 120秒超时
)
if result.returncode != 0:
    print(f"❌ OCR处理失败: {result.stderr}")
```

### 6.3 优雅退出

```python
SHUTDOWN_FLAG = False

def signal_handler(signum, frame):
    global SHUTDOWN_FLAG
    SHUTDOWN_FLAG = True
    print("\n⏸️  收到中断信号，正在优雅退出...")

signal.signal(signal.SIGINT, signal_handler)

# 在长时间任务中检查退出标志
for post in posts:
    if SHUTDOWN_FLAG:
        break
    process_post(post)
```

## 七、使用示例

### 7.1 完整流程（自动执行）

```bash
# 执行example.py，会自动执行所有功能
python example.py
```

### 7.2 单独运行多关键词爬虫

```python
from xhs_crawler.crawlers.multi_keyword_crawler import MultiKeywordCrawler

crawler = MultiKeywordCrawler()
crawler.run(
    keywords=["大模型面试", "面试技巧"],
    max_pages=2,
    page_size=10
)
```

### 7.3 数据清洗

```bash
# 清理指定目录的JSON文件
python xhs_crawler/cleaners/clean_json_files.py ./output

# 自定义要删除的字段
python xhs_crawler/cleaners/clean_json_files.py ./output xsec_token,cookie
```

### 7.4 帖子内容总结

```bash
python xhs_crawler/summarizers/summarize_posts.py
```

### 7.5 提取热门关键词

```python
from xhs_crawler.summarizers.hot_keywords import extract_hot_keywords_from_directory

hot_keywords = extract_hot_keywords_from_directory(
    directory="大模型面试帖子",
    top_n=50  # 提取前50个热门关键词
)
```

## 八、数据库连接配置

### 8.1 环境变量

```bash
# 方式1：直接使用DATABASE_URL
export DATABASE_URL="postgresql://user:password@host:5432/dbname"

# 方式2：分别配置
export NEON_HOST="xxx.neon.tech"
export NEON_PORT="5432"
export NEON_USERNAME="user"
export NEON_PASSWORD="password"
export NEON_DATABASE="dbname"

# 方式3：从VITE_NEON_AUTH_URL提取
export VITE_NEON_AUTH_URL="https://xxx.neonauth.xxx/neondb/auth"
```

### 8.2 上下文管理器使用

```python
from xhs_crawler.core.database import NeonDatabase

# 方式1：使用with语句（推荐）
with NeonDatabase() as db:
    db.upload_content("test.html", html_content)
    # 自动关闭连接

# 方式2：手动管理
db = NeonDatabase()
try:
    db.upload_file("test.html")
finally:
    db.close()
```

## 九、总结

小红书爬虫系统完整处理流程：

1. **配置读取** → 从 `search_config.json` 读取搜索参数
2. **数据采集** → 通过MCP工具调用小红书搜索和详情API
3. **数据清洗** → 删除敏感字段（token、cookie等）
4. **内容增强** → AI分析（总结、情感、关键点提取）
5. **图像处理** → OCR识别图片内容
6. **索引构建** → TF-IDF向量用于相似度搜索
7. **HTML生成** → 生成可浏览的网页
8. **持久存储** → 上传到Neon PostgreSQL数据库

整个系统支持断点续传、优雅退出、并行处理等企业级功能。
